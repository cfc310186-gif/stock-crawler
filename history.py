import requests
import gspread
from oauth2client.service_account import ServiceAccountCredentials
import datetime
from datetime import timedelta
import urllib3
import re 
import time
import yfinance as yf

# --- è¨­å®šå€ ---
SHEET_NAME = "Stock_Data"
JSON_FILE_NAME = "service_account.json"
BASE_URL = "https://fubon-ebrokerdj.fbs.com.tw/z/zg/zgb/zgb0.djhtm?a=9A00&b=0039004100390031&c=B"
DAYS_TO_CRAWL = 30  # è¦å¾€å›æŠ“å¹¾å¤©

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- è¼”åŠ©å‡½å¼ï¼šæŸ¥è©¢"ç‰¹å®šæ—¥æœŸ"çš„è‚¡åƒ¹ ---
def get_historical_price(stock_id, date_str):
    try:
        # yfinance éœ€è¦ä¸‹ä¸€å¤©æ‰èƒ½é–å®šç•¶å¤©ï¼Œä¾‹å¦‚è¦æŠ“ 12-01ï¼Œend å¿…é ˆè¨­ 12-02
        date_obj = datetime.datetime.strptime(date_str, "%Y-%m-%d").date()
        next_day = date_obj + timedelta(days=1)
        next_day_str = next_day.strftime('%Y-%m-%d')

        # 1. å˜—è©¦ä¸Šå¸‚
        ticker = f"{stock_id}.TW"
        data = yf.download(ticker, start=date_str, end=next_day_str, progress=False)
        
        if data.empty:
            # 2. å˜—è©¦ä¸Šæ«ƒ
            ticker = f"{stock_id}.TWO"
            data = yf.download(ticker, start=date_str, end=next_day_str, progress=False)
        
        if not data.empty:
            # å–å¾—ç•¶æ—¥æ”¶ç›¤åƒ¹
            price = data['Close'].iloc[0]
            # è™•ç†æœ‰æ™‚å€™å›å‚³æ˜¯ Series çš„æƒ…æ³
            return float(price) if not isinstance(price, list) else float(price[0])
        else:
            return None 
            
    except Exception as e:
        return None

def crawl_history():
    # æº–å‚™ Google Sheet é€£ç·š
    scope = ["https://spreadsheets.google.com/feeds", 'https://www.googleapis.com/auth/drive']
    creds = ServiceAccountCredentials.from_json_keyfile_name(JSON_FILE_NAME, scope)
    client = gspread.authorize(creds)
    sheet = client.open(SHEET_NAME).sheet1
    
    # è‹¥è¡¨æ ¼å…¨ç©ºï¼Œå…ˆå¯«å…¥æ¨™é¡Œ
    if len(sheet.get_all_values()) == 0:
        header = ["æ—¥æœŸ", "ä»£è™Ÿ", "åç¨±", "è²·è³£åˆ¥", "è²·è³£è¶…é‡‘é¡(åƒ)", "æ”¶ç›¤åƒ¹", "ä¼°ç®—å¼µæ•¸"]
        sheet.append_row(header)

    # è¨­å®šè¿´åœˆï¼šå¾ 30 å¤©å‰é–‹å§‹ï¼Œä¸€è·¯è·‘åˆ°æ˜¨å¤©
    # (ç‚ºä»€éº¼ä¸å«ä»Šå¤©ï¼Ÿå› ç‚ºä»Šå¤©é€šå¸¸æœƒç”± main.py è‡ªå‹•è·‘ï¼Œé¿å…é‡è¤‡)
    today = datetime.date.today()
    
    print(f"ğŸš€ å•Ÿå‹•æ­·å²çˆ¬èŸ²ï¼šé è¨ˆçˆ¬å–éå» {DAYS_TO_CRAWL} å¤©è³‡æ–™...")
    print("--------------------------------------------------")

    # range(30, 0, -1) ä»£è¡¨å¾ 30 å€’æ•¸åˆ° 1
    # é€™æ¨£å¯«å…¥é †åºå°±æ˜¯ï¼š30å¤©å‰ -> 29å¤©å‰ -> ... -> æ˜¨å¤© (æ™‚é–“è»¸ç”±èˆŠåˆ°æ–°)
    for i in range(DAYS_TO_CRAWL, 0, -1):
        target_date = today - timedelta(days=i)
        date_str = target_date.strftime('%Y-%m-%d')
        
        # --- åˆ¤æ–·å‡æ—¥ ---
        # weekday(): 0=é€±ä¸€, 4=é€±äº”, 5=é€±å…­, 6=é€±æ—¥
        if target_date.weekday() >= 5:
            print(f"[{date_str}] æ˜¯é€±æœ« (é€±{'å…­æ—¥'[target_date.weekday()-5]})ï¼Œè‡ªå‹•è·³éã€‚")
            continue

        print(f"\n[{date_str}] æ­£åœ¨è™•ç†ä¸­...")

        # çµ„åˆç¶²å€
        target_url = f"{BASE_URL}&e={date_str}&f={date_str}"
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}

        try:
            # æŠ“å–ç¶²é 
            response = requests.get(target_url, headers=headers, verify=False)
            response.encoding = 'cp950'
            raw_text = response.text
            
            # æ­£è¦è¡¨é”å¼æŠ“å–
            pattern = r"GenLink2stk\('([A-Z0-9]+)','([^']+)'\);[\s\S]*?>([-0-9,]+)<[\s\S]*?>([-0-9,]+)<[\s\S]*?>([-0-9,]+)<"
            matches = re.findall(pattern, raw_text)

            if not matches:
                print(f"   âš ï¸  è©²æ—¥æœŸç„¡è³‡æ–™ (å¯èƒ½æ˜¯åœ‹å®šå‡æ—¥æˆ–é¢±é¢¨å‡)ã€‚")
                time.sleep(1) # ä¼‘æ¯ä¸€ä¸‹å†æ›ä¸‹ä¸€å¤©
                continue

            print(f"   ğŸ” æ‰¾åˆ° {len(matches)} ç­†åˆ†é»è³‡æ–™ï¼Œé–‹å§‹æŸ¥æ­·å²è‚¡åƒ¹...")
            
            daily_data = []
            for m in matches:
                stock_id = m[0].replace('AS', '')
                stock_name = m[1]
                net_amt = int(m[4].replace(',', ''))
                
                # è²·è³£åˆ¥
                if net_amt > 0: status = "è²·è¶…"
                elif net_amt < 0: status = "è³£è¶…"
                else: status = "å¹³"

                # æŸ¥è©¢æ­·å²è‚¡åƒ¹
                price = get_historical_price(stock_id, date_str)
                
                # æ›ç®—å¼µæ•¸
                estimated_sheets = 0
                if price and price > 0:
                    estimated_sheets = int(round(net_amt / price, 0))
                else:
                    estimated_sheets = "N/A"

                row = [date_str, stock_id, stock_name, status, net_amt, price if price else "æŸ¥ç„¡", estimated_sheets]
                daily_data.append(row)

            # å¯«å…¥è©²æ—¥è³‡æ–™ (æ•´æ‰¹å¯«å…¥æ¯”è¼ƒå¿«)
            sheet.append_rows(daily_data)
            print(f"   âœ… å·²å¯«å…¥ {len(daily_data)} ç­†è³‡æ–™ã€‚")

        except Exception as e:
            print(f"   âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")

        # --- é—œéµï¼šä¼‘æ¯æ™‚é–“ ---
        # æ¯è·‘å®Œä¸€å¤©ï¼Œä¼‘æ¯ 3 ç§’ï¼Œé¿å…ä¼ºæœå™¨è¦ºå¾—æˆ‘å€‘æ˜¯æ”»æ“Šè€…
        print("   ğŸ’¤ ä¼‘æ¯ 3 ç§’å¾Œç¹¼çºŒ...")
        time.sleep(3)

    print("\nğŸ‰ æ­·å²è³‡æ–™è£œå®Œè¨ˆç•«åŸ·è¡Œå®Œç•¢ï¼")

if __name__ == "__main__":
    crawl_history()